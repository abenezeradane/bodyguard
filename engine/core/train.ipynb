{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57060e2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install transformers datasets scikit-learn pandas torch\n",
    "!pip install huggingface_hub[hf_xet]\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load data\n",
    "ZIP_PATH = os.path.join(\"engine\", \"data\", \"processed_cyberbullying_dataset.zip\")\n",
    "\n",
    "EXTRACT_DIR = os.path.join(\"engine\", \"data\")\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(EXTRACT_DIR)\n",
    "\n",
    "DATA_DIR = os.path.join(\"engine\", \"data\", \"processed_cyberbullying_dataset.csv\")\n",
    "assert os.path.exists(DATA_DIR), f\"Data file not found: {DATA_DIR}\"\n",
    "df = pd.read_csv(DATA_DIR)\n",
    "df = df[[\"text\", \"binary_label\"]]\n",
    "df[\"label\"] = df[\"binary_label\"].map({\"not_cyberbullying\": 0, \"cyberbullying\": 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a99412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Tokenization\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Dataset preparation\n",
    "train_texts, val_texts = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "train_ds = Dataset.from_pandas(train_texts[[\"text\", \"label\"]])\n",
    "val_ds = Dataset.from_pandas(val_texts[[\"text\", \"label\"]])\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0505788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load pre-trained model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc25a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Handle class imbalance with weights\n",
    "class_counts = df[\"label\"].value_counts().sort_index().values\n",
    "total = class_counts.sum()\n",
    "weights = torch.tensor([total / (2 * c) for c in class_counts], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define custom Trainer with weighted loss\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=weights.to(logits.device))\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11977c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Define training arguments\n",
    "MODEL_DIR = os.path.join(\"model\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb959bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Initialize and train the model\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Rename labels \n",
    "if os.path.exists(MODEL_DIR):\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "    \n",
    "model.config.id2label = {0: \"not_cyberbullying\", 1: \"cyberbullying\"}\n",
    "model.config.label2id = {\"not_cyberbullying\": 0, \"cyberbullying\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b58904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Save model and tokenizer\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0231c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Verify Classifier Weights\n",
    "print(f\"Classifier Weights: {model.classifier.out_proj.weight.mean().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
